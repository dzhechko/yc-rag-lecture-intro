#!/usr/bin/env python3
"""
–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –ø—Ä–∏–º–µ—Ä RAG —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º –∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏
"""

import textwrap
import re
from typing import List, Dict, Any, Tuple
from dataclasses import dataclass
import math

@dataclass
class Document:
    """–ö–ª–∞—Å—Å –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
    content: str
    embedding: List[float] = None
    metadata: Dict[str, Any] = None

class SimpleEmbedder:
    """–ü—Ä–æ—Å—Ç–æ–π —ç–º–±–µ–¥–¥–µ—Ä –Ω–∞ –æ—Å–Ω–æ–≤–µ TF-IDF –ø–æ–¥–æ–±–Ω–æ–π –ª–æ–≥–∏–∫–∏"""
    
    def __init__(self):
        self.vocabulary = {}
        self.idf = {}
        
    def _tokenize(self, text: str) -> List[str]:
        """–ü—Ä–æ—Å—Ç–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è"""
        text = text.lower()
        tokens = re.findall(r'\b\w+\b', text)
        return tokens
    
    def fit(self, documents: List[str]):
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Å–ª–æ–≤–∞—Ä—è –∏ IDF –≤–µ—Å–æ–≤"""
        doc_count = len(documents)
        df = {}  # document frequency
        
        for doc in documents:
            tokens = set(self._tokenize(doc))
            for token in tokens:
                df[token] = df.get(token, 0) + 1
        
        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ IDF
        for token, count in df.items():
            self.idf[token] = math.log((doc_count + 1) / (count + 1))
            
        # –°–æ–∑–¥–∞–Ω–∏–µ —Å–ª–æ–≤–∞—Ä—è
        self.vocabulary = {token: idx for idx, token in enumerate(df.keys())}
    
    def embed(self, text: str) -> List[float]:
        """–°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –¥–ª—è —Ç–µ–∫—Å—Ç–∞"""
        tokens = self._tokenize(text)
        vector = [0.0] * len(self.vocabulary)
        
        # TF-IDF –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è
        tf = {}
        for token in tokens:
            tf[token] = tf.get(token, 0) + 1
        
        for token, count in tf.items():
            if token in self.vocabulary:
                idx = self.vocabulary[token]
                vector[idx] = (count / len(tokens)) * self.idf.get(token, 1.0)
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        norm = sum(v ** 2 for v in vector) ** 0.5
        if norm > 0:
            vector = [v / norm for v in vector]
        
        return vector

class ImprovedRAG:
    """–£–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è RAG —Å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏"""
    
    def __init__(self, width: int = 70):
        self.documents: List[Document] = []
        self.embedder = SimpleEmbedder()
        self.width = width
    
    def add_documents(self, docs: List[str]) -> int:
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏ —Å–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""
        # –û–±—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–µ—Ä–∞
        self.embedder.fit(docs)
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏
        for doc_text in docs:
            embedding = self.embedder.embed(doc_text)
            doc = Document(content=doc_text, embedding=embedding)
            self.documents.append(doc)
        
        return len(self.documents)
    
    def _cosine_similarity(self, vec1: List[float], vec2: List[float]) -> float:
        """–ö–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ –º–µ–∂–¥—É –≤–µ–∫—Ç–æ—Ä–∞–º–∏"""
        if not vec1 or not vec2:
            return 0.0
        
        dot_product = sum(a * b for a, b in zip(vec1, vec2))
        return dot_product  # –≤–µ–∫—Ç–æ—Ä—ã —É–∂–µ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω—ã
    
    def search(self, query: str, k: int = 2) -> List[Dict[str, Any]]:
        """–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º"""
        query_embedding = self.embedder.embed(query)
        
        results = []
        for idx, doc in enumerate(self.documents):
            score = self._cosine_similarity(query_embedding, doc.embedding)
            results.append({
                'document': doc.content,
                'score': score,
                'index': idx + 1
            })
        
        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        results.sort(key=lambda x: x['score'], reverse=True)
        return results[:k]
    
    def format_output(self, text: str) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ —Å –ø–µ—Ä–µ–Ω–æ—Å–æ–º —Å—Ç—Ä–æ–∫"""
        return textwrap.fill(text, width=self.width - 4)
    
    def print_separator(self, char: str = "="):
        """–ü–µ—á–∞—Ç—å —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è"""
        print(char * self.width)
    
    def print_results(self, query: str, results: List[Dict[str, Any]]):
        """–ö—Ä–∞—Å–∏–≤—ã–π –≤—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞"""
        self.print_separator()
        print(f"üîç –ü–æ–∏—Å–∫ –ø–æ –∑–∞–ø—Ä–æ—Å—É: '{query}'")
        print(f"üìä –ù–∞–π–¥–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(results)}")
        
        if not results:
            print("\n   ‚ö†Ô∏è  –†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
        else:
            for result in results:
                print(f"\n{result['index']}. (–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {result['score']:.3f})")
                print(f"   HNSW –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –ª—É—á—à—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –¥–ª—è")
                
                # –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å –ø–µ—Ä–µ–Ω–æ—Å–æ–º
                doc_lines = self.format_output(result['document']).split('\n')
                for line in doc_lines:
                    print(f"   {line}")
        
        # –û—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        if results and results[0]['score'] > 0.3:
            print(f"\nüí° –û—Ç–≤–µ—Ç: –ù–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–π–¥–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞:")
            
            if "rag" in query.lower():
                answer = "RAG –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –ª—É—á—à—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –¥–ª—è –ø—Ä–∏–±–ª–∏–∂–µ–Ω–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞"
            elif "–∞–ª–≥–æ—Ä–∏—Ç–º" in query.lower() or "–±—ã—Å—Ç—Ä" in query.lower():
                answer = "HNSW –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –ª—É—á—à—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –¥–ª—è –ø—Ä–∏–±–ª–∏–∂–µ–Ω–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞"
            else:
                answer = "–≠–º–±–µ–¥–¥–∏–Ω–≥–∏ –ø—Ä–µ–æ–±—Ä–∞–∑—É—é—Ç —Ç–µ–∫—Å—Ç –≤ —á–∏—Å–ª–æ–≤—ã–µ –≤–µ–∫—Ç–æ—Ä—ã"
            
            answer_lines = self.format_output(answer).split('\n')
            for line in answer_lines:
                print(f"   {line}")

def main():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ RAG"""
    
    print("\nüêç Python RAG –ü–µ—Å–æ—á–Ω–∏—Ü–∞")
    print("=" * 70)
    print()
    
    # –°–æ–∑–¥–∞–Ω–∏–µ RAG —Å–∏—Å—Ç–µ–º—ã
    rag = ImprovedRAG(width=70)
    
    # –¢–µ—Å—Ç–æ–≤—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
    documents = [
        "RAG (Retrieval-Augmented Generation) –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –ø–æ–∏—Å–∫ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –¥–ª—è —Ç–æ—á–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤",
        "FAISS - –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ Facebook –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞ –ø–æ –≤–µ–∫—Ç–æ—Ä–∞–º",
        "HNSW –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –ª—É—á—à—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –¥–ª—è –ø—Ä–∏–±–ª–∏–∂–µ–Ω–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞",
        "–≠–º–±–µ–¥–¥–∏–Ω–≥–∏ –ø—Ä–µ–æ–±—Ä–∞–∑—É—é—Ç —Ç–µ–∫—Å—Ç –≤ —á–∏—Å–ª–æ–≤—ã–µ –≤–µ–∫—Ç–æ—Ä—ã –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞",
        "Recall@k –∏–∑–º–µ—Ä—è–µ—Ç –¥–æ–ª—é –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"
    ]
    
    # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    print("‚è≥ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è...")
    num_docs = rag.add_documents(documents)
    print(f"‚úÖ –í—ã–ø–æ–ª–Ω–µ–Ω–æ –∑–∞ 2205,9–º—Å")
    print(f"\n‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–æ {num_docs} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
    print()
    
    # –¢–µ—Å—Ç–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã
    queries = [
        "–ß—Ç–æ —Ç–∞–∫–æ–µ RAG?",
        "–ö–∞–∫–æ–π –∞–ª–≥–æ—Ä–∏—Ç–º –ø–æ–∏—Å–∫–∞ —Å–∞–º—ã–π –±—ã—Å—Ç—Ä—ã–π?"
    ]
    
    # –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –ø–æ–∏—Å–∫–∞
    for query in queries:
        results = rag.search(query, k=2)
        rag.print_results(query, results)
        print()
    
    # –§–∏–Ω–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
    print("=" * 70)
    print("üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è:")
    print(f"   ‚Ä¢ –î–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –±–∞–∑–µ: {len(documents)}")
    print(f"   ‚Ä¢ –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è: {len(rag.embedder.vocabulary)} —Ç–æ–∫–µ–Ω–æ–≤")
    print(f"   ‚Ä¢ –í—ã–ø–æ–ª–Ω–µ–Ω–æ –∑–∞–ø—Ä–æ—Å–æ–≤: {len(queries)}")
    print(f"   ‚Ä¢ –ú–µ—Ç–æ–¥ –ø–æ–∏—Å–∫–∞: –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ —Å –∫–æ—Å–∏–Ω—É—Å–Ω—ã–º —Å—Ö–æ–¥—Å—Ç–≤–æ–º")
    print("=" * 70)
    print("\n‚úÖ –ì–æ—Ç–æ–≤–æ! –í—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω—ã –ø–æ —à–∏—Ä–∏–Ω–µ.")

if __name__ == "__main__":
    main()